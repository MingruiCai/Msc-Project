{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "colab": {
   "name": "tweepy_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('python38': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "f4252c4dcc56551aaff3144c1bf9955e95133a7a1a373139ec6b46ae17c7a042"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tweepy  # https://github.com/tweepy/tweepy\r\n",
    "import csv\r\n",
    "screen_name=\"EdinburghUni\"\r\n",
    "# Twitter API credentials\r\n",
    "consumer_key = \"tndAcVLSnfdGVxHXSrtCx7yY0\"\r\n",
    "consumer_secret = \"dXZD2WAHW6vfQIo1L77HLGp9SmvETn058sMaXaQoh9Zm7HcQD3\"\r\n",
    "access_key = \"1405286036473843714-QVmhQZVHpSPzICwqjXokxVoSTRslb5\"\r\n",
    "access_secret = \"7gBfupUaKCM5boYQYJK1YYDEvpILhTTIWr3ESXGnS7gOQ\"\r\n",
    "\r\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\r\n",
    "auth.set_access_token(access_key, access_secret)\r\n",
    "api = tweepy.API(auth)\r\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "2PzYvKoSzM4p"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# initialize a list to hold all the tweepy Tweets\r\n",
    "alltweets = []\r\n",
    "\r\n",
    "# make initial request for most recent tweets (200 is the maximum allowed count)\r\n",
    "new_tweets = api.user_timeline(screen_name=screen_name, count=200,tweet_mode=\"extended\")\r\n",
    "alltweets.extend(new_tweets)\r\n",
    "\r\n",
    "# save the id of the oldest tweet less one\r\n",
    "oldest = alltweets[-1].id - 1\r\n",
    "print(len(alltweets))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "200\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_O6dv2rgsN-",
    "outputId": "0006fb55-e4d4-4922-f4ff-04f07d0006c4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\r\n",
    "# You can only get a maximum of 200 tweets in one request. \r\n",
    "# However, you can make successive requests for older tweets. The maximum number of tweets that you can get in a timeline is 3200.\r\n",
    "while len(new_tweets) > 0:\r\n",
    "    \r\n",
    "    # all subsiquent requests use the max_id param to prevent duplicates\r\n",
    "    new_tweets = api.user_timeline(screen_name=screen_name, count=200, max_id=oldest,tweet_mode=\"extended\")\r\n",
    "\r\n",
    "    # save most recent tweets\r\n",
    "    alltweets.extend(new_tweets)\r\n",
    "\r\n",
    "    # update the id of the oldest tweet less one\r\n",
    "    oldest = alltweets[-1].id -1\r\n",
    "print(len(alltweets))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3202\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WzhpiRrcez7f",
    "outputId": "24368a21-20c7-42dc-aed0-4b1ce388093c"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#extract the place name from the timeline\r\n",
    "placeset=set()\r\n",
    "for tweet in alltweets :\r\n",
    "    if tweet.place is not None and tweet.place.place_type==\"city\":\r\n",
    "      # print(tweet.place.place_type)\r\n",
    "      placeset.add(tweet.place.name)\r\n",
    "placeslist=list(placeset)\r\n",
    "print(placeslist)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Edinburgh', 'Berlin']\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6rUEFJwqzax0",
    "outputId": "e8a040ef-6ae3-482a-cddd-a84469bf247d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "from nltk.probability import FreqDist\r\n",
    "hashtags=[]\r\n",
    "for tweet in alltweets:\r\n",
    "  hashtaginfo=tweet.entities.get(\"hashtags\")\r\n",
    "  if len(hashtaginfo)!=0:\r\n",
    "    hashtags.append(hashtaginfo[0].get(\"text\").lower())\r\n",
    "# print(hashtags)\r\n",
    "tag_freq = FreqDist(hashtags)\r\n",
    "print(tag_freq.most_common())\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method Counter.most_common of FreqDist({'edinburghgrad': 92, 'covid19': 53, 'edinburghimpact': 35, 'jamestaitblack': 33, 'sharingthingspodcast': 29, 'edwelcome': 26, 'edopenday': 22, 'doorsopenday': 20, 'edinburgh': 19, 'livinggratefully': 18, ...})>\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RsAQ88MWaU8u",
    "outputId": "80da1562-9920-497f-b409-b4fa6f17deba"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "# divide tags into first class and second class\r\n",
    "tag2class=[tag[0] for tag in tag_freq.most_common() if tag[1]>=5] # high possibility\r\n",
    "tag1class=[tag[0] for tag in tag2class if tag[1]>=10] # normal possibility\r\n",
    "print(tag1class)\r\n",
    "print(tag2class)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['edinburghgrad', 'covid19', 'edinburghimpact', 'jamestaitblack', 'sharingthingspodcast', 'edwelcome', 'edopenday', 'doorsopenday', 'edinburgh', 'livinggratefully', 'internationalwomensday', 'edscifest', 'edinburghfuturesconversations', 'eduniedscifest', 'drawntoedinburgh', 'worldheritageday', 'womenshistorymonth']\n",
      "['edinburghgrad', 'covid19', 'edinburghimpact', 'jamestaitblack', 'sharingthingspodcast', 'edwelcome', 'edopenday', 'doorsopenday', 'edinburgh', 'livinggratefully', 'internationalwomensday', 'edscifest', 'edinburghfuturesconversations', 'eduniedscifest', 'drawntoedinburgh', 'worldheritageday', 'womenshistorymonth', 'europeday', 'studentvolunteeringweek', 'womeninscience', '1daywithoutus', 'gbbo', 'internationaldayofwomenandgirlsinscience', 'edinburghseven', 'clearing', 'scotappweek20', 'gew2019', 'africaweek', 'fcl18', 'universitychallenge', 'wideningparticipation', 'volunteersweek', 'nhs', 'internationalstudentsday', 'sv19', 'giffordlecture', 'worldoceansday', 'cancer', 'worldveterinaryday', 'standrewsday', 'africaweek2020', 'europeandayoflanguages', 'museumweek2020', 'brexit', 'womeninstem', 'fairtradefortnight', 'lgbthm', 'chinesenewyear', 'edinburghwelcome', 'adalovelaceday', 'worldcancerday', 'dodscot']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "outtweets = [tweet.full_text for tweet in alltweets]\r\n",
    "# print(outtweets)\r\n",
    "del(outtweets[0])\r\n",
    "# print(outtweets)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-g9D1s0f4iml",
    "outputId": "4d40745f-fc94-4dfe-c2ea-14d012ccf663"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import re\r\n",
    "import snowballstemmer\r\n",
    "#  remove stop words\r\n",
    "stopwords = []\r\n",
    "with open(\"stopwords.txt\", \"r\") as f1:\r\n",
    "    for stop_word in f1.readlines():\r\n",
    "        if stop_word is not None:  # read stop words from a txt file and store then into a list.\r\n",
    "            stopwords.append(stop_word.strip(\"\\n\"))\r\n",
    "\r\n",
    "\r\n",
    "# preprocess used for builidng inverted index including tokenisation, casefolding, removing stop words and porter stemming.\r\n",
    "def preprocess(text):\r\n",
    "    remove_url = re.sub(r\"http\\S+\", \"\", text)\r\n",
    "    # remove_number=remove_url.replace('\\d+', '')\r\n",
    "    regEx = re.compile(\"\\W\").split(remove_url)  # split non-letter characters and store them into a list.\r\n",
    "    token = [i for i in regEx if i != '']\r\n",
    "    casefolding = [s.lower() for s in token if isinstance(s, str) == True]  # all in lower case.\r\n",
    "    stopwords_removed = [w for w in casefolding if not w in stopwords]  # removing stop words.\r\n",
    "    stemmer = snowballstemmer.stemmer('english')  # Porter stemmer by using snowball stemmer lib.\r\n",
    "    return stemmer.stemWords(stopwords_removed)\r\n",
    "\r\n",
    "preprocessed_tweets=[]\r\n",
    "for tweet in outtweets:\r\n",
    "  preprocessed_tweets.append(preprocess(tweet))\r\n",
    "# print(preprocessed_tweets)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QWpUF_x01AZp",
    "outputId": "0eee96df-1163-42e6-ea93-fb041a84e16d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#word frequency\r\n",
    "flat_words = [item for sublist in preprocessed_tweets for item in sublist]\r\n",
    "# print(flat_words)\r\n",
    "\r\n",
    "word_freq = FreqDist(flat_words)\r\n",
    "print(word_freq.most_common(10))\r\n",
    "\r\n",
    "from gensim.corpora import Dictionary\r\n",
    "\r\n",
    "#create dictionary\r\n",
    "text_dict = Dictionary(preprocessed_tweets)\r\n",
    "\r\n",
    "#view integer mappings\r\n",
    "# print(text_dict.token2id)\r\n",
    "\r\n",
    "tweets_bow = [text_dict.doc2bow(tweet) for tweet in preprocessed_tweets]\r\n",
    "# print(tweets_bow)\r\n",
    "\r\n",
    "from gensim.models.ldamodel import LdaModel\r\n",
    "#fitting lda model\r\n",
    "tweets_lda = LdaModel(tweets_bow,\r\n",
    "                      num_topics = 10,\r\n",
    "                      id2word = text_dict,\r\n",
    "                      random_state=1,\r\n",
    "                      # passes=10\r\n",
    "                     )\r\n",
    "\r\n",
    "x=tweets_lda.show_topics()\r\n",
    "alltopicWords=[]\r\n",
    "for topicNum,topicWords in x:\r\n",
    "  words_foreachtopic = re.findall(\"[a-zA-Z]+\",x[topicNum][1])\r\n",
    "  alltopicWords+=words_foreachtopic\r\n",
    "alltopicWordsset=set(alltopicWords)\r\n",
    "alltopicWords=list(alltopicWordsset)\r\n",
    "print(alltopicWords)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('amp', 905), ('student', 738), ('edinburghuni', 479), ('edinburgh', 478), ('rt', 461), ('univers', 446), ('find', 432), ('research', 424), ('studi', 327), ('year', 299)]\n",
      "['student', 'find', 'uk', 'world', 'ceremoni', 'school', 'talbotrice', 'live', 'join', 'exhibit', 'work', 'eca', 'scientist', 'peopl', 'edinburgh', 'year', 'contact', 'congratul', 'challeng', 'part', 'edinburghuni', 'research', 'geosciences', 'edinburghgrad', 'suggest', 'graduat', 'life', 'award', 'studi', 'today', 'museum', 'book', 'open', 'amp', 'day', 'univers', 'lectur', 'team', 'rt', 'event', 'develop', 'offer']\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOcFpMUR1e3D",
    "outputId": "680c1a95-fdbe-4aff-90a1-5d130f7c75e6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "\r\n",
    "newtweet=\"Berlin\"\r\n",
    "# !pip install geotext\r\n",
    "# Determine if a city name exists because this python library only supports city name recognition but the tweet place entity can have place type other than city such as admin \r\n",
    "from geotext import GeoText\r\n",
    "# 1. recognize the city name in the new tweet\r\n",
    "placename=GeoText(newtweet).cities\r\n",
    "if len(placename)!=0:\r\n",
    "  if set(placename)<=set(placeslist):\r\n",
    "    print(\"This new tweet is fine to post !\")\r\n",
    "\r\n",
    " "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This new tweet is fine to post !\n"
     ]
    }
   ],
   "metadata": {
    "id": "SjcKjHaXCM3F",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e3bf8164-bbf4-4a62-fd62-f34c810f45c7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "# 2. Extract the hashtag words\r\n",
    "# set a threshold (word frequency) rare or not \r\n",
    "\r\n",
    "# function to print all the hashtags in a text\r\n",
    "def extract_hashtags(text):\r\n",
    "    # initializing hashtag_list variable\r\n",
    "    hashtag_list = []\r\n",
    "\r\n",
    "    # splitting the text into words\r\n",
    "    for word in text.lower().split():\r\n",
    "\r\n",
    "        # checking the first charcter of every word\r\n",
    "        if word[0] == '#':\r\n",
    "            if word[1:]!=\"\":\r\n",
    "              # adding the word to the hashtag_list\r\n",
    "              hashtag_list.append(word[1:].lower())\r\n",
    "    # If all the hashtags of new tweet has appeared in previous tweets, then we decide to pass it\r\n",
    "    \r\n",
    "        \r\n",
    "    return hashtag_list\r\n",
    "    # printing the hashtag_list\r\n",
    "    # print(\"The hashtags in \\\"\" + text + \"\\\" are :\")\r\n",
    "    # for hashtag in hashtag_list:\r\n",
    "    #     print(hashtag)"
   ],
   "outputs": [],
   "metadata": {
    "id": "_n61l0PGmwZL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "text2 = \"This day is beautiful ! #edinburghgrad #forest # cute\"\r\n",
    "if set(extract_hashtags(text2))<=set(tag2class):\r\n",
    "  print(\"This tweet is fine to post!\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This tweet is fine to post!\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddcEoaMbvGC-",
    "outputId": "d7f667db-1a8e-4201-ee2b-06f2785d7b05"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# 3. Determine if the number of key tokens in new tweets is over the threshold\r\n",
    "testtweet=\"Banning in-car smoking when children are present can reduce their exposure to tobacco smoke by more than 30 per cent, new research from @EdinUniUsher shows.\"\r\n",
    "txt=preprocess(testtweet)\r\n",
    "counter=0\r\n",
    "for token in txt:\r\n",
    "  if token in alltopicWords :\r\n",
    "    counter+=1\r\n",
    "    print(token) \r\n",
    "if counter>=round(len(alltopicWords)/10):\r\n",
    "  print(counter)\r\n",
    "  print(\"This tweet is fine to post!\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "research\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mLYQvvEFrL0T",
    "outputId": "a8de8085-d155-44fa-f9a5-66d0a75e4161"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# 4.using lsi to measure the similarity of new tweet and old corpus\r\n",
    "#if there are previous tweets that get over 0.97 score, then we can say the new tweet is similar to some previous tweets \r\n",
    "corpus=tweets_bow\r\n",
    "id2word=text_dict\r\n",
    "dictionary=text_dict\r\n",
    "from gensim import models\r\n",
    "lsi = models.LsiModel(corpus=tweets_bow, id2word=text_dict, num_topics=10)\r\n",
    "\r\n",
    "doc = 'We are aware of the issues some graduates are having trying to access the booking system. We are looking into this and are trying to resolve this as soon as possible, We apologise for the inconvenience.'\r\n",
    "# doc='Booking is now open!'\r\n",
    "vec_bow = dictionary.doc2bow(preprocess(doc))\r\n",
    "vec_lsi = lsi[vec_bow]  # convert the query to LSI space\r\n",
    "# print(vec_lsi)\r\n",
    "\r\n",
    "from gensim import similarities\r\n",
    "index = similarities.MatrixSimilarity(lsi[corpus])  # transform corpus to LSI space and index it\r\n",
    "\r\n",
    "index.save('deerwester.index')\r\n",
    "index = similarities.MatrixSimilarity.load('deerwester.index')\r\n",
    "\r\n",
    "sims = index[vec_lsi]  # perform a similarity query against the corpus\r\n",
    "# print(list(enumerate(sims)))  # print (document_number, document_similarity) 2-tuples\r\n",
    "\r\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\r\n",
    "for doc_position, doc_score in sims:\r\n",
    "    if doc_score>=0.97:\r\n",
    "      print(doc_score, outtweets[doc_position])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0 We are aware of the issues some graduates are having trying to access the booking system. We are looking into this and are trying to resolve this as soon as possible, We apologise for the inconvenience.\n"
     ]
    }
   ],
   "metadata": {
    "id": "HS5mWBSCN-l9"
   }
  }
 ]
}